{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370a9085-0fd6-4900-9f0a-31d42d962a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e3ae04-9865-4ac1-b7d6-46593eb768e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "\n",
    "    def __init__(self, func, leaky_val=0.01):\n",
    "        self.func = func\n",
    "        self.leaky_val = leaky_val\n",
    "\n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return (1/(1+ (math.e **(-x))))\n",
    "        \n",
    "    def _tanh(self, x):\n",
    "        return (2 *  self._sigmoid(2*x) -1)\n",
    "        \n",
    "    def _reLU(self, x):\n",
    "        return max(0,x)\n",
    "\n",
    "    def _leakyReLU(self, x):\n",
    "        if(x > 0):\n",
    "            return x\n",
    "        return self.leaky_val*x\n",
    "\n",
    "\n",
    "    def gradient(self, x):\n",
    "        if self.func == 'linear':\n",
    "            return 1\n",
    "        elif self.func == 'sig':\n",
    "            calc = self._sigmoid(x)\n",
    "            return (calc * (1 - calc))\n",
    "        elif self.func == 'tanh':\n",
    "            return 1 - (self._tanh(x)**2)\n",
    "        elif self.func == 'reLU':\n",
    "            if x > 0:\n",
    "                return 1\n",
    "            return 0\n",
    "        elif self.func == 'leaky_reLU':\n",
    "            if x > 0:\n",
    "                return 1\n",
    "            return self.leaky_val\n",
    "        else:\n",
    "            raise ValueError(\"The only available activation functions are: linear, sig, tanh, reLU and leaky_reLU.\")\n",
    "    \n",
    "    def calculate(self, x):\n",
    "        if self.func == 'linear':\n",
    "            return self._linear(x)\n",
    "        elif self.func == 'sig':\n",
    "            return self._sigmoid(x)\n",
    "        elif self.func == 'tanh':\n",
    "            return self._tanh(x)\n",
    "        elif self.func == 'reLU':\n",
    "            return self._reLU(x)\n",
    "        elif self.func == 'leaky_reLU':\n",
    "            return self._leakyReLU(x)\n",
    "        else:\n",
    "            raise ValueError(\"The only available activation functions are: linear, sig, tanh, reLU and leaky_reLU.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1b75ff-48e7-45ef-a06f-d4e7ac149ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "\n",
    "    def __init__(self, func='mse'):\n",
    "        self.func = func\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        if(self.func == 'mse'):\n",
    "            return self._MSE(y_pred, y_true)\n",
    "        elif (self.func == 'mae'):\n",
    "            return self._MAE(y_pred, y_true)\n",
    "        elif (self.func == 'binary_ce'):\n",
    "            return self._binary_CE(y_pred, y_true)\n",
    "        else:\n",
    "            raise Exception(\"only supported loss function options are: mse, mae and binary_ce\")\n",
    "            \n",
    "\n",
    "    def gradient(self, y_pred, y_true):\n",
    "        if (self.func == 'mse'):\n",
    "            return (2 / y_pred.size) * (y_pred - y_true)\n",
    "        elif (self.func == 'mae'):\n",
    "            return np.sign(y_pred - y_true)\n",
    "        elif(self.func == 'binary_ce'):\n",
    "            return (y_pred - y_true)/(y_pred*(1-y_pred))\n",
    "            \n",
    "        \n",
    "    def _MSE(self, y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def _MAE(self, y_pred, y_true):\n",
    "        return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "    def _binary_CE(self, y_pred, y_true):\n",
    "        epsilon = 1e-12\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        ce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43709c4-1606-460e-9e8d-103324593e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25\n",
      "[-1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "loss = LossFunction('mae')\n",
    "x = loss.calculate(np.array([1,0.5]), np.array([5,7]))\n",
    "print(x)\n",
    "x = loss.gradient(np.array([10,12,11]), np.array([12,1,115]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8305776-7b23-4fcd-802b-dede993f363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10\n"
     ]
    }
   ],
   "source": [
    "x = ActivationFunction('linear')\n",
    "print(x.calculate(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5913c6-4cdd-4c3e-a55c-2dda5908fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, input_amt, func, leaky_val=0.01, learning_rate=0.01):\n",
    "        self.weights = np.random.randn(input_amt) * 0.01\n",
    "        self.bias = 0\n",
    "        self.delta = 0\n",
    "        self.z = 0\n",
    "        self.activation_output_z = 0\n",
    "        self.last_inputs = np.ones(input_amt)\n",
    "        self.activation_func = ActivationFunction(func=func, leaky_val=leaky_val)\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.last_inputs = inputs.copy()\n",
    "        self.z = np.dot(inputs, self.weights) + self.bias\n",
    "        self.activation_output_z = self.activation_func.calculate(self.z)\n",
    "\n",
    "        \n",
    "        return self.activation_output_z\n",
    "\n",
    "    def get_z(self):\n",
    "        return self.z\n",
    "        \n",
    "    def get_delta(self):\n",
    "        return self.delta\n",
    "\n",
    "    def update_parameters(self, delta):\n",
    "        self.delta = delta\n",
    "        grad = self.delta * self.last_inputs\n",
    "        # Clip gradients\n",
    "        grad = np.clip(grad, -1.0, 1.0)\n",
    "    \n",
    "        self.weights -= self.learning_rate * grad\n",
    "        self.bias -= self.learning_rate * self.delta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c927c1-84e8-4dd8-a534-41c194c05660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006199278941855247\n"
     ]
    }
   ],
   "source": [
    "n = Neuron(input_amt=2,func='reLU')\n",
    "print(n.forward([1,-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4c84681-9f3f-42cb-9b90-732751ccdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, network_shape:tuple, activation_funct=\"sig\", leaky_val=0.01, loss=\"mse\", learning_rate=0.01):\n",
    "\n",
    "        self.shape = list(network_shape)\n",
    "        self.network = []\n",
    "        self.loss_func = LossFunction(loss)\n",
    "        self.input_size = network_shape[0] # the input amount -----------\n",
    "        self.activation_func = ActivationFunction(func=activation_funct, leaky_val=leaky_val)\n",
    "        \n",
    "        for i in range(1, len(network_shape)): #Append every other layer\n",
    "            self.network.append([Neuron(network_shape[i-1], func=activation_funct,leaky_val=leaky_val, learning_rate=learning_rate) for _ in range(network_shape[i])])\n",
    "\n",
    "    def fit(self, x, y, epochs=100):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for feature, target in zip(x,y):\n",
    "                predictions = self._forward_pass(feature)                \n",
    "                self._back_propogation(predictions, target)\n",
    "\n",
    "                loss = self.loss_func.calculate(predictions, target)\n",
    "                total_loss += loss\n",
    "                \n",
    "            print(f\"At Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        # x is expected to be 2D: (n_samples, n_features)\n",
    "        predictions = np.zeros((x.shape[0], 1))  # Pre-allocate\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            predictions[i] = self._forward_pass(x[i])  # Process one sample\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def _forward_pass(self, inputs: np.ndarray):\n",
    "        if len(inputs) != self.input_size:\n",
    "            raise Exception(f\"Err. Input size must be{self.input_size} as set earlier.\")\n",
    "\n",
    "        self.input_layer = inputs\n",
    "        current = inputs.copy()\n",
    "        for i in range(len(self.network)): #for each layer ------------------------\n",
    "\n",
    "            returned_vals = []\n",
    "            for n in self.network[i]: #Calculate the values returned by each neuron at the given layer ---\n",
    "                returned_vals.append(n.forward(current))\n",
    "\n",
    "            current = np.array(returned_vals)\n",
    "        \n",
    "        return current\n",
    "\n",
    "    def _back_propogation(self, predicted_vals, true_vals):\n",
    "        true_vals = np.atleast_1d(true_vals)\n",
    "    \n",
    "        loss_gradient = self.loss_func.gradient(predicted_vals, true_vals)\n",
    "    \n",
    "        for i in range(len(self.network) - 1, -1, -1):\n",
    "            prev_loss_gradient = None\n",
    "            if i > 0:\n",
    "                prev_loss_gradient = np.zeros(len(self.network[i - 1]))\n",
    "    \n",
    "            for j, n in enumerate(self.network[i]):\n",
    "                z = n.get_z()\n",
    "    \n",
    "                # Output layer delta\n",
    "                delta = loss_gradient[j] * n.activation_func.gradient(z)\n",
    "    \n",
    "                old_weights = n.weights.copy()\n",
    "                n.update_parameters(delta)\n",
    "    \n",
    "                if i > 0:\n",
    "                    for k in range(len(old_weights)):\n",
    "                        prev_loss_gradient[k] += delta * old_weights[k]\n",
    "    \n",
    "            if i > 0:\n",
    "                loss_gradient = prev_loss_gradient\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95386f0e-2fb8-4a45-944e-45a99f067fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a linear dataset to test my NN -------------\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=1000,      # Number of samples\n",
    "    n_features=8,       # Number of features\n",
    "    n_informative=6,    # Number of useful features\n",
    "    n_targets=1,        # Number of regression targets\n",
    "    bias=5,          # Bias term (intercept) in the linear model\n",
    "    noise=0,         # Standard deviation of the Gaussian noise\n",
    "    random_state=42     # Set a random state for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0463987f-b7f6-4423-82c6-b0e71ef323a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef36aed0-8b7c-4243-a4e7-c4e3cf40906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network((8,1), activation_funct=\"linear\", loss=\"mse\", learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e6c5577-8d30-4b32-88b3-86567cd2bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff9db4be-bf23-40d1-a580-d2bb3ac18d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38aa6437-2729-4ffc-8ee0-137bb725620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch 1, Loss: 16460577.094997358\n",
      "At Epoch 2, Loss: 15343175.963890633\n",
      "At Epoch 3, Loss: 14262890.26476725\n",
      "At Epoch 4, Loss: 13222147.510088881\n",
      "At Epoch 5, Loss: 12221299.720803486\n",
      "At Epoch 6, Loss: 11258880.88232293\n",
      "At Epoch 7, Loss: 10334725.21661411\n",
      "At Epoch 8, Loss: 9448465.130167376\n",
      "At Epoch 9, Loss: 8600591.71104828\n",
      "At Epoch 10, Loss: 7792333.739946226\n",
      "At Epoch 11, Loss: 7022162.690675963\n",
      "At Epoch 12, Loss: 6291720.833632362\n",
      "At Epoch 13, Loss: 5601997.581365442\n",
      "At Epoch 14, Loss: 4952615.956252799\n",
      "At Epoch 15, Loss: 4343432.374258048\n",
      "At Epoch 16, Loss: 3775101.810940007\n",
      "At Epoch 17, Loss: 3247809.779949878\n",
      "At Epoch 18, Loss: 2761534.44513606\n",
      "At Epoch 19, Loss: 2315532.3193123084\n",
      "At Epoch 20, Loss: 1909610.3737759215\n",
      "At Epoch 21, Loss: 1544659.6684357745\n",
      "At Epoch 22, Loss: 1220647.7807645372\n",
      "At Epoch 23, Loss: 936467.7986272187\n",
      "At Epoch 24, Loss: 691356.0052310849\n",
      "At Epoch 25, Loss: 484181.4360278323\n",
      "At Epoch 26, Loss: 314668.37536607543\n",
      "At Epoch 27, Loss: 182204.1904312262\n",
      "At Epoch 28, Loss: 86157.00736122217\n",
      "At Epoch 29, Loss: 26440.92598587726\n",
      "At Epoch 30, Loss: 1987.9803060737825\n",
      "At Epoch 31, Loss: 0.00023745915161544774\n",
      "At Epoch 32, Loss: 1.9567539526654357e-17\n",
      "At Epoch 33, Loss: 7.155094628099412e-25\n",
      "At Epoch 34, Loss: 5.392787254444724e-25\n",
      "At Epoch 35, Loss: 5.153517853282139e-25\n",
      "At Epoch 36, Loss: 5.1807098886851075e-25\n",
      "At Epoch 37, Loss: 5.173121046776881e-25\n",
      "At Epoch 38, Loss: 5.15016519443495e-25\n",
      "At Epoch 39, Loss: 5.149912758945279e-25\n",
      "At Epoch 40, Loss: 5.149912758945279e-25\n",
      "At Epoch 41, Loss: 5.149912758945279e-25\n",
      "At Epoch 42, Loss: 5.149912758945279e-25\n",
      "At Epoch 43, Loss: 5.147388404048572e-25\n",
      "At Epoch 44, Loss: 5.161469571206767e-25\n",
      "At Epoch 45, Loss: 5.170336367781451e-25\n",
      "At Epoch 46, Loss: 5.187809636832097e-25\n",
      "At Epoch 47, Loss: 5.2140865935850085e-25\n",
      "At Epoch 48, Loss: 5.17261617579754e-25\n",
      "At Epoch 49, Loss: 5.160641267256285e-25\n",
      "At Epoch 50, Loss: 5.188992928189928e-25\n",
      "At Epoch 51, Loss: 5.120251588908969e-25\n",
      "At Epoch 52, Loss: 5.168340549691242e-25\n",
      "At Epoch 53, Loss: 5.190807308271936e-25\n",
      "At Epoch 54, Loss: 5.140525314173149e-25\n",
      "At Epoch 55, Loss: 5.136249688066851e-25\n",
      "At Epoch 56, Loss: 5.178098759088826e-25\n",
      "At Epoch 57, Loss: 5.1775938881094845e-25\n",
      "At Epoch 58, Loss: 5.1775938881094845e-25\n",
      "At Epoch 59, Loss: 5.1775938881094845e-25\n",
      "At Epoch 60, Loss: 5.185876927614305e-25\n",
      "At Epoch 61, Loss: 5.224468003097717e-25\n",
      "At Epoch 62, Loss: 5.149912758945279e-25\n",
      "At Epoch 63, Loss: 5.147388404048572e-25\n",
      "At Epoch 64, Loss: 5.176678809459428e-25\n",
      "At Epoch 65, Loss: 5.150046865299167e-25\n",
      "At Epoch 66, Loss: 5.147388404048572e-25\n",
      "At Epoch 67, Loss: 5.1807098886851075e-25\n",
      "At Epoch 68, Loss: 5.17261617579754e-25\n",
      "At Epoch 69, Loss: 5.160641267256285e-25\n",
      "At Epoch 70, Loss: 5.188992928189928e-25\n",
      "At Epoch 71, Loss: 5.120251588908969e-25\n",
      "At Epoch 72, Loss: 5.168340549691242e-25\n",
      "At Epoch 73, Loss: 5.190807308271936e-25\n",
      "At Epoch 74, Loss: 5.140525314173149e-25\n",
      "At Epoch 75, Loss: 5.136249688066851e-25\n",
      "At Epoch 76, Loss: 5.178098759088826e-25\n",
      "At Epoch 77, Loss: 5.1775938881094845e-25\n",
      "At Epoch 78, Loss: 5.1775938881094845e-25\n",
      "At Epoch 79, Loss: 5.1775938881094845e-25\n",
      "At Epoch 80, Loss: 5.185876927614305e-25\n",
      "At Epoch 81, Loss: 5.224468003097717e-25\n",
      "At Epoch 82, Loss: 5.149912758945279e-25\n",
      "At Epoch 83, Loss: 5.147388404048572e-25\n",
      "At Epoch 84, Loss: 5.176678809459428e-25\n",
      "At Epoch 85, Loss: 5.149912758945279e-25\n",
      "At Epoch 86, Loss: 5.1475225104024595e-25\n",
      "At Epoch 87, Loss: 5.1807098886851075e-25\n",
      "At Epoch 88, Loss: 5.17261617579754e-25\n",
      "At Epoch 89, Loss: 5.160641267256285e-25\n",
      "At Epoch 90, Loss: 5.188992928189928e-25\n",
      "At Epoch 91, Loss: 5.120251588908969e-25\n",
      "At Epoch 92, Loss: 5.168340549691242e-25\n",
      "At Epoch 93, Loss: 5.190807308271936e-25\n",
      "At Epoch 94, Loss: 5.140525314173149e-25\n",
      "At Epoch 95, Loss: 5.136249688066851e-25\n",
      "At Epoch 96, Loss: 5.166983708934262e-25\n",
      "At Epoch 97, Loss: 5.181191093837292e-25\n",
      "At Epoch 98, Loss: 5.136249688066851e-25\n",
      "At Epoch 99, Loss: 5.166983708934262e-25\n",
      "At Epoch 100, Loss: 5.181191093837292e-25\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79a8a8ae-c1e3-482e-b144-429f328916e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing sigmoid --------------------------------------------------\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c63cfa92-8ee4-47e3-b90e-4b620e19c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch 1, Loss: 553.7462276707521\n",
      "At Epoch 2, Loss: 245.19974714121665\n",
      "At Epoch 3, Loss: 67.06259831986853\n",
      "At Epoch 4, Loss: 45.12030173229146\n",
      "At Epoch 5, Loss: 38.086179690335406\n",
      "At Epoch 6, Loss: 34.808960287127185\n",
      "At Epoch 7, Loss: 32.98680115946654\n",
      "At Epoch 8, Loss: 31.861520536432106\n",
      "At Epoch 9, Loss: 31.115771261703497\n",
      "At Epoch 10, Loss: 30.595529478706837\n",
      "At Epoch 11, Loss: 30.218006826765784\n",
      "At Epoch 12, Loss: 29.935256817541735\n",
      "At Epoch 13, Loss: 29.71788337603662\n",
      "At Epoch 14, Loss: 29.547035090916673\n",
      "At Epoch 15, Loss: 29.41017472295972\n",
      "At Epoch 16, Loss: 29.298709593459947\n",
      "At Epoch 17, Loss: 29.206599357005974\n",
      "At Epoch 18, Loss: 29.129504740642666\n",
      "At Epoch 19, Loss: 29.06424916989017\n",
      "At Epoch 20, Loss: 29.00846827605559\n",
      "At Epoch 21, Loss: 28.960375925777466\n",
      "At Epoch 22, Loss: 28.91860457693947\n",
      "At Epoch 23, Loss: 28.882094229447794\n",
      "At Epoch 24, Loss: 28.850013847792518\n",
      "At Epoch 25, Loss: 28.821704906502724\n",
      "At Epoch 26, Loss: 28.79664027142395\n",
      "At Epoch 27, Loss: 28.77439387888166\n",
      "At Epoch 28, Loss: 28.75461812529073\n",
      "At Epoch 29, Loss: 28.737026833254227\n",
      "At Epoch 30, Loss: 28.72138229792845\n",
      "At Epoch 31, Loss: 28.70748535078149\n",
      "At Epoch 32, Loss: 28.69516767664307\n",
      "At Epoch 33, Loss: 28.68428582867307\n",
      "At Epoch 34, Loss: 28.674716533497925\n",
      "At Epoch 35, Loss: 28.66635298434817\n",
      "At Epoch 36, Loss: 28.65910189633538\n",
      "At Epoch 37, Loss: 28.65288115368516\n",
      "At Epoch 38, Loss: 28.647617919736174\n",
      "At Epoch 39, Loss: 28.643247110938137\n",
      "At Epoch 40, Loss: 28.639710158851578\n",
      "At Epoch 41, Loss: 28.636954001295923\n",
      "At Epoch 42, Loss: 28.634930256804868\n",
      "At Epoch 43, Loss: 28.633594546474807\n",
      "At Epoch 44, Loss: 28.632905934918682\n",
      "At Epoch 45, Loss: 28.63282646791977\n",
      "At Epoch 46, Loss: 28.633320788951288\n",
      "At Epoch 47, Loss: 28.634355820289183\n",
      "At Epoch 48, Loss: 28.635900497238165\n",
      "At Epoch 49, Loss: 28.63792554619069\n",
      "At Epoch 50, Loss: 28.64040329897469\n",
      "At Epoch 51, Loss: 28.643307537331484\n",
      "At Epoch 52, Loss: 28.64661336246049\n",
      "At Epoch 53, Loss: 28.650297085458007\n",
      "At Epoch 54, Loss: 28.65433613518258\n",
      "At Epoch 55, Loss: 28.658708980658158\n",
      "At Epoch 56, Loss: 28.663395065592216\n",
      "At Epoch 57, Loss: 28.668374752971637\n",
      "At Epoch 58, Loss: 28.67362927801248\n",
      "At Epoch 59, Loss: 28.679140708001896\n",
      "At Epoch 60, Loss: 28.68489190779016\n",
      "At Epoch 61, Loss: 28.690866509872436\n",
      "At Epoch 62, Loss: 28.697048888158886\n",
      "At Epoch 63, Loss: 28.703424134663056\n",
      "At Epoch 64, Loss: 28.709978038455137\n",
      "At Epoch 65, Loss: 28.716697066326763\n",
      "At Epoch 66, Loss: 28.723568344701697\n",
      "At Epoch 67, Loss: 28.73057964240415\n",
      "At Epoch 68, Loss: 28.737719353965677\n",
      "At Epoch 69, Loss: 28.74497648320966\n",
      "At Epoch 70, Loss: 28.752340626910826\n",
      "At Epoch 71, Loss: 28.759801958368854\n",
      "At Epoch 72, Loss: 28.76735121078352\n",
      "At Epoch 73, Loss: 28.774979660349672\n",
      "At Epoch 74, Loss: 28.782679109028432\n",
      "At Epoch 75, Loss: 28.790441866972916\n",
      "At Epoch 76, Loss: 28.79826073461551\n",
      "At Epoch 77, Loss: 28.806128984439436\n",
      "At Epoch 78, Loss: 28.81410243808321\n",
      "At Epoch 79, Loss: 28.82221431102251\n",
      "At Epoch 80, Loss: 28.830325937320232\n",
      "At Epoch 81, Loss: 28.838467833647485\n",
      "At Epoch 82, Loss: 28.846636530526002\n",
      "At Epoch 83, Loss: 28.85482576595291\n",
      "At Epoch 84, Loss: 28.86302978195626\n",
      "At Epoch 85, Loss: 28.8712435578121\n",
      "At Epoch 86, Loss: 28.87946272747165\n",
      "At Epoch 87, Loss: 28.887683483045063\n",
      "At Epoch 88, Loss: 28.89590249368456\n",
      "At Epoch 89, Loss: 28.904116839456133\n",
      "At Epoch 90, Loss: 28.912323957235785\n",
      "At Epoch 91, Loss: 28.92052159603788\n",
      "At Epoch 92, Loss: 28.92870777973237\n",
      "At Epoch 93, Loss: 28.936880775579112\n",
      "At Epoch 94, Loss: 28.945039067362675\n",
      "At Epoch 95, Loss: 28.953181332194003\n",
      "At Epoch 96, Loss: 28.96130642026134\n",
      "At Epoch 97, Loss: 28.969413336975265\n",
      "At Epoch 98, Loss: 28.977501227082612\n",
      "At Epoch 99, Loss: 28.98556936041776\n",
      "At Epoch 100, Loss: 28.993617119035402\n",
      "At Epoch 101, Loss: 29.00164398552497\n",
      "At Epoch 102, Loss: 29.009649532347524\n",
      "At Epoch 103, Loss: 29.017633412071064\n",
      "At Epoch 104, Loss: 29.025595348402614\n",
      "At Epoch 105, Loss: 29.033535127934343\n",
      "At Epoch 106, Loss: 29.041452592536384\n",
      "At Epoch 107, Loss: 29.049347632338556\n",
      "At Epoch 108, Loss: 29.05722017925115\n",
      "At Epoch 109, Loss: 29.065070200984206\n",
      "At Epoch 110, Loss: 29.07289769552467\n",
      "At Epoch 111, Loss: 29.080702686041697\n",
      "At Epoch 112, Loss: 29.08848521618657\n",
      "At Epoch 113, Loss: 29.096245345762384\n",
      "At Epoch 114, Loss: 29.103983146736688\n",
      "At Epoch 115, Loss: 29.111698699576177\n",
      "At Epoch 116, Loss: 29.119392089879852\n",
      "At Epoch 117, Loss: 29.12706340529324\n",
      "At Epoch 118, Loss: 29.134712732684836\n",
      "At Epoch 119, Loss: 29.142340155568274\n",
      "At Epoch 120, Loss: 29.14994575175444\n",
      "At Epoch 121, Loss: 29.157529591221238\n",
      "At Epoch 122, Loss: 29.1650917341844\n",
      "At Epoch 123, Loss: 29.172632229361398\n",
      "At Epoch 124, Loss: 29.18015111241532\n",
      "At Epoch 125, Loss: 29.187648404567113\n",
      "At Epoch 126, Loss: 29.19512411137128\n",
      "At Epoch 127, Loss: 29.202578221640284\n",
      "At Epoch 128, Loss: 29.210010706515593\n",
      "At Epoch 129, Loss: 29.217421518674158\n",
      "At Epoch 130, Loss: 29.22481059166438\n",
      "At Epoch 131, Loss: 29.232177839366713\n",
      "At Epoch 132, Loss: 29.239523155570193\n",
      "At Epoch 133, Loss: 29.24684641366234\n",
      "At Epoch 134, Loss: 29.25414746642492\n",
      "At Epoch 135, Loss: 29.261426145931633\n",
      "At Epoch 136, Loss: 29.268682263542814\n",
      "At Epoch 137, Loss: 29.275915609992637\n",
      "At Epoch 138, Loss: 29.283125955564017\n",
      "At Epoch 139, Loss: 29.29031305034781\n",
      "At Epoch 140, Loss: 29.297476624581204\n",
      "At Epoch 141, Loss: 29.304616389061565\n",
      "At Epoch 142, Loss: 29.311732035631785\n",
      "At Epoch 143, Loss: 29.318823237732705\n",
      "At Epoch 144, Loss: 29.325889651019658\n",
      "At Epoch 145, Loss: 29.332930914036968\n",
      "At Epoch 146, Loss: 29.339946648949148\n",
      "At Epoch 147, Loss: 29.346936462322528\n",
      "At Epoch 148, Loss: 29.353899945953703\n",
      "At Epoch 149, Loss: 29.360836677742267\n",
      "At Epoch 150, Loss: 29.367746222601706\n",
      "At Epoch 151, Loss: 29.374628133405057\n",
      "At Epoch 152, Loss: 29.381481951962495\n",
      "At Epoch 153, Loss: 29.38830721002471\n",
      "At Epoch 154, Loss: 29.39510343030883\n",
      "At Epoch 155, Loss: 29.401870127544147\n",
      "At Epoch 156, Loss: 29.40860680953123\n",
      "At Epoch 157, Loss: 29.41531297821205\n",
      "At Epoch 158, Loss: 29.421988130746804\n",
      "At Epoch 159, Loss: 29.428631760593724\n",
      "At Epoch 160, Loss: 29.435243358587382\n",
      "At Epoch 161, Loss: 29.44182241401269\n",
      "At Epoch 162, Loss: 29.4483684156705\n",
      "At Epoch 163, Loss: 29.454880852931222\n",
      "At Epoch 164, Loss: 29.461359216773676\n",
      "At Epoch 165, Loss: 29.467803000805173\n",
      "At Epoch 166, Loss: 29.474211702260696\n",
      "At Epoch 167, Loss: 29.48058482297771\n",
      "At Epoch 168, Loss: 29.48692187034301\n",
      "At Epoch 169, Loss: 29.493222358211717\n",
      "At Epoch 170, Loss: 29.499485807792976\n",
      "At Epoch 171, Loss: 29.505711748502595\n",
      "At Epoch 172, Loss: 29.511899718778825\n",
      "At Epoch 173, Loss: 29.518049266860693\n",
      "At Epoch 174, Loss: 29.524159951528034\n",
      "At Epoch 175, Loss: 29.530231342799084\n",
      "At Epoch 176, Loss: 29.53626302258799\n",
      "At Epoch 177, Loss: 29.542254585318933\n",
      "At Epoch 178, Loss: 29.548205638496885\n",
      "At Epoch 179, Loss: 29.554115803234627\n",
      "At Epoch 180, Loss: 29.5599847147363\n",
      "At Epoch 181, Loss: 29.5658120227354\n",
      "At Epoch 182, Loss: 29.57159739188939\n",
      "At Epoch 183, Loss: 29.577340502131253\n",
      "At Epoch 184, Loss: 29.583041048975506\n",
      "At Epoch 185, Loss: 29.588698743783585\n",
      "At Epoch 186, Loss: 29.594313313985396\n",
      "At Epoch 187, Loss: 29.599884503260945\n",
      "At Epoch 188, Loss: 29.60541207168084\n",
      "At Epoch 189, Loss: 29.610895795808357\n",
      "At Epoch 190, Loss: 29.616335468763456\n",
      "At Epoch 191, Loss: 29.621730900250338\n",
      "At Epoch 192, Loss: 29.627081916550328\n",
      "At Epoch 193, Loss: 29.632388360481404\n",
      "At Epoch 194, Loss: 29.6376500913247\n",
      "At Epoch 195, Loss: 29.64286698472265\n",
      "At Epoch 196, Loss: 29.648038932546708\n",
      "At Epoch 197, Loss: 29.65316584273985\n",
      "At Epoch 198, Loss: 29.658247639132544\n",
      "At Epoch 199, Loss: 29.663284261236566\n",
      "At Epoch 200, Loss: 29.668275664015763\n"
     ]
    }
   ],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "nn = Network((2,3,1), activation_funct=\"sig\", loss=\"binary_ce\", learning_rate=0.05)\n",
    "nn.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d767aa8-fdba-4f14-ab5a-d4734f60dda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
